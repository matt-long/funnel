{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute dimension reductions on CESM-LE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot write to data cache '/glade/p/cesmdata/cseg'. Will not be able to download remote data files. Use environment variable 'CESMDATAROOT' to specify another directory.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import cftime\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.util import add_cyclic_point\n",
    "import cmocean\n",
    "\n",
    "import intake\n",
    "import pop_tools\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_file = './data/glade-cesm1-le.json'\n",
    "variables = ['SST', 'Chl_surf', 'IFRAC', 'KGP']\n",
    "experiments = ['20C', 'RCP85']\n",
    "stream = 'pop.h'\n",
    "component = 'ocn'\n",
    "require_ocn_bgc = True\n",
    "\n",
    "mask_name = 'krill-ToE'\n",
    "\n",
    "chunks = {'time': 60}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/mclong/miniconda3/envs/funnel/lib/python3.7/site-packages/distributed/dashboard/core.py:79: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33417112ba7b4e5aa5b5527a3e293bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>NCARCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ncar_jobqueue import NCARCluster\n",
    "cluster = NCARCluster()\n",
    "cluster.scale(36)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://128.117.181.211:42118</li>\n",
       "  <li><b>Dashboard: </b><a href='https://jupyterhub.ucar.edu/dav/user/mclong/proxy/34256/status' target='_blank'>https://jupyterhub.ucar.edu/dav/user/mclong/proxy/34256/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://128.117.181.211:42118' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tornado.application - ERROR - Uncaught exception GET /status/ws (::1)\n",
      "HTTPServerRequest(protocol='http', host='jupyterhub.ucar.edu', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='::1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/glade/work/mclong/miniconda3/envs/funnel/lib/python3.7/site-packages/tornado/websocket.py\", line 956, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "  File \"/glade/work/mclong/miniconda3/envs/funnel/lib/python3.7/site-packages/bokeh/server/views/ws.py\", line 123, in open\n",
      "    raise ProtocolError(\"Subprotocol header is not 'bokeh'\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Subprotocol header is not 'bokeh'\n",
      "tornado.application - ERROR - Uncaught exception GET /status/ws (::1)\n",
      "HTTPServerRequest(protocol='http', host='jupyterhub.ucar.edu', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='::1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/glade/work/mclong/miniconda3/envs/funnel/lib/python3.7/site-packages/tornado/websocket.py\", line 956, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "  File \"/glade/work/mclong/miniconda3/envs/funnel/lib/python3.7/site-packages/bokeh/server/views/ws.py\", line 123, in open\n",
      "    raise ProtocolError(\"Subprotocol header is not 'bokeh'\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Subprotocol header is not 'bokeh'\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(cluster) # Connect this local process to remote workers\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a region mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:      (nlat: 384, nlon: 320, region: 3)\n",
       "Coordinates:\n",
       "  * region       (region) &lt;U14 &#x27;Southern Ocean&#x27; &#x27;WAP &amp; Atlantic&#x27; &#x27;Indo-Pacific&#x27;\n",
       "Dimensions without coordinates: nlat, nlon\n",
       "Data variables:\n",
       "    masked_area  (region, nlat, nlon) float64 dask.array&lt;chunksize=(2, 192, 160), meta=np.ndarray&gt;</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:      (nlat: 384, nlon: 320, region: 3)\n",
       "Coordinates:\n",
       "  * region       (region) <U14 'Southern Ocean' 'WAP & Atlantic' 'Indo-Pacific'\n",
       "Dimensions without coordinates: nlat, nlon\n",
       "Data variables:\n",
       "    masked_area  (region, nlat, nlon) float64 dask.array<chunksize=(2, 192, 160), meta=np.ndarray>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_name = 'POP_gx1v6'\n",
    "\n",
    "nb_parameters = dict(\n",
    "    mask_name=mask_name,\n",
    "    grid_name=grid_name,\n",
    ")\n",
    "\n",
    "# call _pop_region_mask.ipynb(**parameters)\n",
    "# TODO: make this a return value\n",
    "zarr_name = f'./data/region-mask-{grid_name}-{mask_name}.zarr'\n",
    "\n",
    "masked_area = xr.open_zarr(zarr_name)\n",
    "masked_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the CESM-LE data \n",
    "\n",
    "We will use [`intake-esm`](https://intake-esm.readthedocs.io/en/latest/), which is a data catalog tool.\n",
    "It enables querying a database for the files we want, then loading those directly as an `xarray.Dataset`.\n",
    "\n",
    "First step is to set the \"collection\" for the CESM-LE, which depends on a json file conforming to the [ESM Catalog Specification](https://github.com/NCAR/esm-collection-spec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glade-cesm1-le-ESM Collection with 191066 entries:\n",
       "\t> 7 experiment(s)\n",
       "\n",
       "\t> 108 case(s)\n",
       "\n",
       "\t> 6 component(s)\n",
       "\n",
       "\t> 15 stream(s)\n",
       "\n",
       "\t> 1052 variable(s)\n",
       "\n",
       "\t> 116 date_range(s)\n",
       "\n",
       "\t> 40 member_id(s)\n",
       "\n",
       "\t> 191066 path(s)\n",
       "\n",
       "\t> 6 ctrl_branch_year(s)\n",
       "\n",
       "\t> 4 ctrl_experiment(s)\n",
       "\n",
       "\t> 41 ctrl_member_id(s)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = intake.open_esm_datastore(catalog_file)\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select ensemble members\n",
    "\n",
    "Ocean biogeochemistry was corrupted in some members and the data deleted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 101, 102, 103, 104, 105]\n"
     ]
    }
   ],
   "source": [
    "if require_ocn_bgc:\n",
    "    query = dict(\n",
    "        experiment=['20C'],\n",
    "        variable=['diatChl'],\n",
    "    )\n",
    "else:\n",
    "    query = dict(\n",
    "        experiment=['20C'],\n",
    "    )\n",
    "col_sub = col.search(**query)\n",
    "\n",
    "member_id = list(col_sub.df.member_id.unique())    \n",
    "print(member_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process variable list\n",
    "\n",
    "Define functions for derived variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chl_surf(ds):\n",
    "    \"\"\"compute surface chl\"\"\"\n",
    "\n",
    "    ds['Chl_surf'] = (ds.diatChl + ds.spChl + ds.diazChl).isel(z_t_150m=0)\n",
    "    ds.Chl_surf.attrs = ds.diatChl.attrs\n",
    "    ds.Chl_surf.attrs['long_name'] = 'Surface chlorophyll'\n",
    "\n",
    "    for v in ['diatChl', 'spChl', 'diazChl']:\n",
    "        if v not in variables:\n",
    "            ds = ds.drop(v)\n",
    "            \n",
    "    return ds\n",
    "\n",
    "\n",
    "def compute_kgp(ds):\n",
    "    \"\"\"Compute Krill Growth Potential \n",
    "    \n",
    "    Natural growth rates in Antarctic krill (Euphausia superba): II. Predictive \n",
    "    models based on food, temperature, body length, sex, and maturity \n",
    "    stage doi: 10.4319/lo.2006.51.2.0973 \n",
    "    A Atkinson, RS Shreeve, AG Hirst, P Rothery, GA Tarling \n",
    "    Limnol Oceanogr, 2006 \n",
    "    \n",
    "    Oceanic circumpolar habitats of Antarctic krill \n",
    "    doi: 10.3354/meps07498 \n",
    "    A Atkinson, V Siegel, EA Pakhomov, P Rothery, V Loeb \n",
    "    Mar Ecol Prog Ser, 2008\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # specify coefs\n",
    "    a = -0.066\n",
    "    b = 0.002\n",
    "    c = -0.000061\n",
    "    d = 0.385\n",
    "    e = 0.328\n",
    "    f = 0.0078\n",
    "    g = 0.0101\n",
    "    \n",
    "    # local pointers\n",
    "    sst = ds.SST\n",
    "    chl = ds.Chl_surf\n",
    "    \n",
    "    # mask chl with lower bound\n",
    "    chl = chl.where(chl >= 0.5).fillna(0.)\n",
    "        \n",
    "    # length coordinate\n",
    "    length = xr.DataArray(\n",
    "        [20., 40., 60.], \n",
    "        name='length',\n",
    "        dims=('length'), \n",
    "        attrs={\n",
    "            'units': 'mm', \n",
    "            'long_name': \n",
    "            'Krill body length'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute terms and sum\n",
    "    length_term = a + (b * length) + (c * length**2)\n",
    "    chl_term = d * chl / (e + chl)    \n",
    "    sst_term = f * sst + g * sst**2    \n",
    "    kgp = length_term + chl_term + sst_term\n",
    "    kgp.name = 'KGP'\n",
    "    \n",
    "    # mask based on SST range\n",
    "    kgp = kgp.where((-1. <= sst) & (sst <= 5.)).fillna(0.).where(ds.KMT > 0)\n",
    "    \n",
    "    # add coordinates\n",
    "    kgp = kgp.assign_coords({'length': length})\n",
    "    kgp = kgp.assign_coords({'TLONG': ds.TLONG, 'TLAT': ds.TLAT})\n",
    "\n",
    "    # add attrs\n",
    "    kgp.attrs = {'units': 'mm d$^{-1}$', 'long_name': 'Daily growth rate'}\n",
    "    ds['KGP'] = kgp\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorize requested variables as derived or directly written out by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052 variables in catalog\n"
     ]
    }
   ],
   "source": [
    "defined_model_variables = list(col.df.variable.unique())\n",
    "print(f'{len(defined_model_variables)} variables in catalog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_variables: ['IFRAC', 'SST', 'diatChl', 'diazChl', 'spChl']\n",
      "derived_variables: ['Chl_surf', 'KGP']\n"
     ]
    }
   ],
   "source": [
    "defined_derived_variables = {\n",
    "    'Chl_surf': {\n",
    "        'dependencies': ['diatChl', 'spChl', 'diazChl'],\n",
    "        'function': compute_chl_surf,\n",
    "    },\n",
    "    'KGP': {\n",
    "        'dependencies': ['SST', 'Chl_surf'],\n",
    "        'function': compute_kgp,\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "def manage_var_dep(var_list):\n",
    "    \"\"\"Determine if a variable is written directly by \n",
    "       the model (and therefore just read-in)\n",
    "       or derived from dependencies.\n",
    "    \"\"\"\n",
    "    query_variables = []    \n",
    "    derived_variables = []\n",
    "    for v in var_list:    \n",
    "\n",
    "        if v in defined_model_variables:\n",
    "            query_variables.append(v)\n",
    "            \n",
    "        elif v in defined_derived_variables:\n",
    "            q, d = manage_var_dep(\n",
    "                defined_derived_variables[v]['dependencies']\n",
    "            )\n",
    "            derived_variables.append(v)\n",
    "            query_variables.extend(q)\n",
    "        else:    \n",
    "            raise ValueError(f'unknown variable {v}')    \n",
    "\n",
    "    return (\n",
    "        sorted(list(set(query_variables))), \n",
    "        sorted(list(set(derived_variables)))\n",
    "    )\n",
    "\n",
    "query_variables, derived_variables = manage_var_dep(variables)\n",
    "print(f'query_variables: {query_variables}')\n",
    "print(f'derived_variables: {derived_variables}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for the queried data\n",
    "\n",
    "Specify a list of variables and perform a search. Under the hood, the `search` functionality uses [`pandas`](https://pandas.pydata.org/) data frames. We can view that frame here using the `.df` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glade-cesm1-le-ESM Collection with 475 entries:\n",
      "\t> 2 experiment(s)\n",
      "\n",
      "\t> 68 case(s)\n",
      "\n",
      "\t> 1 component(s)\n",
      "\n",
      "\t> 1 stream(s)\n",
      "\n",
      "\t> 5 variable(s)\n",
      "\n",
      "\t> 5 date_range(s)\n",
      "\n",
      "\t> 34 member_id(s)\n",
      "\n",
      "\t> 475 path(s)\n",
      "\n",
      "\t> 3 ctrl_branch_year(s)\n",
      "\n",
      "\t> 2 ctrl_experiment(s)\n",
      "\n",
      "\t> 34 ctrl_member_id(s)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>case</th>\n",
       "      <th>component</th>\n",
       "      <th>stream</th>\n",
       "      <th>variable</th>\n",
       "      <th>date_range</th>\n",
       "      <th>member_id</th>\n",
       "      <th>path</th>\n",
       "      <th>ctrl_branch_year</th>\n",
       "      <th>ctrl_experiment</th>\n",
       "      <th>ctrl_member_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20C</td>\n",
       "      <td>b.e11.B20TRC5CNBDRD.f09_g16.001</td>\n",
       "      <td>ocn</td>\n",
       "      <td>pop.h</td>\n",
       "      <td>IFRAC</td>\n",
       "      <td>185001-200512</td>\n",
       "      <td>1</td>\n",
       "      <td>/glade/campaign/cesm/collections/cesmLE/CESM-C...</td>\n",
       "      <td>402</td>\n",
       "      <td>CTRL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20C</td>\n",
       "      <td>b.e11.B20TRC5CNBDRD.f09_g16.002</td>\n",
       "      <td>ocn</td>\n",
       "      <td>pop.h</td>\n",
       "      <td>IFRAC</td>\n",
       "      <td>192001-200512</td>\n",
       "      <td>2</td>\n",
       "      <td>/glade/campaign/cesm/collections/cesmLE/CESM-C...</td>\n",
       "      <td>1920</td>\n",
       "      <td>20C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20C</td>\n",
       "      <td>b.e11.B20TRC5CNBDRD.f09_g16.009</td>\n",
       "      <td>ocn</td>\n",
       "      <td>pop.h</td>\n",
       "      <td>IFRAC</td>\n",
       "      <td>192001-200512</td>\n",
       "      <td>9</td>\n",
       "      <td>/glade/campaign/cesm/collections/cesmLE/CESM-C...</td>\n",
       "      <td>1920</td>\n",
       "      <td>20C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20C</td>\n",
       "      <td>b.e11.B20TRC5CNBDRD.f09_g16.010</td>\n",
       "      <td>ocn</td>\n",
       "      <td>pop.h</td>\n",
       "      <td>IFRAC</td>\n",
       "      <td>192001-200512</td>\n",
       "      <td>10</td>\n",
       "      <td>/glade/campaign/cesm/collections/cesmLE/CESM-C...</td>\n",
       "      <td>1920</td>\n",
       "      <td>20C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20C</td>\n",
       "      <td>b.e11.B20TRC5CNBDRD.f09_g16.011</td>\n",
       "      <td>ocn</td>\n",
       "      <td>pop.h</td>\n",
       "      <td>IFRAC</td>\n",
       "      <td>192001-200512</td>\n",
       "      <td>11</td>\n",
       "      <td>/glade/campaign/cesm/collections/cesmLE/CESM-C...</td>\n",
       "      <td>1920</td>\n",
       "      <td>20C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experiment                             case component stream variable  \\\n",
       "0        20C  b.e11.B20TRC5CNBDRD.f09_g16.001       ocn  pop.h    IFRAC   \n",
       "1        20C  b.e11.B20TRC5CNBDRD.f09_g16.002       ocn  pop.h    IFRAC   \n",
       "2        20C  b.e11.B20TRC5CNBDRD.f09_g16.009       ocn  pop.h    IFRAC   \n",
       "3        20C  b.e11.B20TRC5CNBDRD.f09_g16.010       ocn  pop.h    IFRAC   \n",
       "4        20C  b.e11.B20TRC5CNBDRD.f09_g16.011       ocn  pop.h    IFRAC   \n",
       "\n",
       "      date_range  member_id  \\\n",
       "0  185001-200512          1   \n",
       "1  192001-200512          2   \n",
       "2  192001-200512          9   \n",
       "3  192001-200512         10   \n",
       "4  192001-200512         11   \n",
       "\n",
       "                                                path  ctrl_branch_year  \\\n",
       "0  /glade/campaign/cesm/collections/cesmLE/CESM-C...               402   \n",
       "1  /glade/campaign/cesm/collections/cesmLE/CESM-C...              1920   \n",
       "2  /glade/campaign/cesm/collections/cesmLE/CESM-C...              1920   \n",
       "3  /glade/campaign/cesm/collections/cesmLE/CESM-C...              1920   \n",
       "4  /glade/campaign/cesm/collections/cesmLE/CESM-C...              1920   \n",
       "\n",
       "  ctrl_experiment  ctrl_member_id  \n",
       "0            CTRL               1  \n",
       "1             20C               1  \n",
       "2             20C               1  \n",
       "3             20C               1  \n",
       "4             20C               1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_sub = col.search(\n",
    "    experiment=experiments, \n",
    "    stream=stream, \n",
    "    variable=query_variables,\n",
    "    member_id=member_id,\n",
    "    )\n",
    "\n",
    "print(col_sub)\n",
    "\n",
    "col_sub.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the [`to_dataset_dict`](https://intake-esm.readthedocs.io/en/latest/api.html#intake_esm.core.esm_datastore.to_dataset_dict) method to return a dictionary of `xarray.Dataset`'s. \n",
    "\n",
    "`intake_esm` makes groups of these according to rules in the collection spec file.\n",
    "\n",
    "We can use the `preprocess` parameter to pass in a function that makes some corrections to the dataset. So first we define a function that does the following:\n",
    "- drop the singleton dimension on SST (which screws up coordinate alignment)\n",
    "- make the 2D grid vars coordinates\n",
    "- [add your correction here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "    \"\"\"Fix some things in the dataset and subset in space\n",
    "       This is POP-centric.       \n",
    "    \"\"\"   \n",
    "    \n",
    "    # TODO: make this component model agnostic\n",
    "    \n",
    "    grid_vars = ['KMT', 'TAREA', 'TLAT', 'TLONG', 'z_t', 'dz', 'z_t_150m', 'time', 'time_bound']\n",
    "    \n",
    "    if 'SST' in ds:\n",
    "        ds['SST'] = ds.SST.isel(z_t=0, drop=True)\n",
    "    return ds\n",
    "             \n",
    "    data_vars = list(\n",
    "        filter(\n",
    "            lambda v: v in query_variables, \n",
    "            ds.data_vars\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ds = ds[data_vars+grid_vars]  \n",
    "    \n",
    "    # TODO: this could be a good place to apply domain subsetting\n",
    "    \n",
    "    # set grid variables to coordinates to ease concatenation in intake-esm\n",
    "    new_coords = set(grid_vars) - set(ds.coords)\n",
    "\n",
    "    return ds.set_coords(new_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'component.experiment.stream'\n",
      "                \n",
      "--> There is/are 2 group(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tornado.application - ERROR - Uncaught exception GET /status/ws (::1)\n",
      "HTTPServerRequest(protocol='http', host='jupyterhub.ucar.edu', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='::1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/glade/work/mclong/miniconda3/envs/funnel/lib/python3.7/site-packages/tornado/websocket.py\", line 956, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "  File \"/glade/work/mclong/miniconda3/envs/funnel/lib/python3.7/site-packages/bokeh/server/views/ws.py\", line 123, in open\n",
      "    raise ProtocolError(\"Subprotocol header is not 'bokeh'\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Subprotocol header is not 'bokeh'\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dset_orig = col_sub.to_dataset_dict(\n",
    "    cdf_kwargs={\n",
    "        'chunks': chunks, \n",
    "        'decode_times': False},\n",
    "    preprocess=preprocess\n",
    ")\n",
    "dsets_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply post-query corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(ds):\n",
    "    ds = ds.copy(deep=True)\n",
    "    ds['time'] = xr.DataArray(cftime.num2date(ds.time_bound.mean(dim='d2'), units=ds.time.units, \n",
    "                                              calendar=ds.time.calendar), dims=('time'))\n",
    "    return ds \n",
    "\n",
    "dsets = {key: fix_time(ds) for key, ds in dsets_orig.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute derived variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if derived_variables:\n",
    "    for v in derived_variables:\n",
    "        func = defined_derived_variables[v]['function']\n",
    "        dsets = {key: func(ds) for key, ds in dsets.items()}\n",
    "\n",
    "dsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the datasets in time, i.e. 20C + RCP8.5 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generalize this, we need the notion of \"concatenatable groups\"\n",
    "\n",
    "ordered_dsets_keys = ['ocn.20C.pop.h', 'ocn.RCP85.pop.h']\n",
    "\n",
    "ds = xr.concat(\n",
    "    [dsets[exp] for exp in ordered_dsets_keys], \n",
    "    dim='time', \n",
    "    data_vars='minimal'\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute summer time (DJF) means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_djf = util.ann_mean(ds, season='DJF', time_bnds_varname='time_bound')\n",
    "ds_djf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute regional means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dim = ['nlat', 'nlon']\n",
    "area_total = masked_area.sum(dim)\n",
    "weights = masked_area / area_total\n",
    "weights_sum = weights.sum(dim)\n",
    "\n",
    "# ensure that the weights add to 1.\n",
    "np.testing.assert_allclose(weights_sum.where(weights_sum != 0.).fillna(1.), 1.0, rtol=1e-7)\n",
    "\n",
    "with xr.set_options(keep_attrs=True):\n",
    "    ds_djf_regional = (ds_djf[list(ds_djf.data_vars)] * weights).sum(dim).compute()\n",
    "ds_djf_regional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick look plots for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_region in masked_area.region.values:\n",
    "\n",
    "    nvar = len(ds_djf_regional.data_vars)\n",
    "    ncol = int(np.sqrt(nvar))\n",
    "    nrow = int(nvar/ncol) + min(1, nvar%ncol)\n",
    "\n",
    "    fig, ax = plt.subplots(nrow, ncol, figsize=(4*ncol, 3*nrow),\n",
    "                           constrained_layout=True)\n",
    "\n",
    "    for i, v in enumerate(ds_djf_regional.data_vars):\n",
    "        plt.axes(ax.ravel()[i])\n",
    "\n",
    "        var = ds_djf_regional[v].sel(region=plot_region)\n",
    "        if 'length' in var.dims:\n",
    "            var = var.sel(length=40.)            \n",
    "        for m_id in ds_djf_regional.member_id:\n",
    "            var_i = var.sel(member_id=m_id)\n",
    "            var_i.plot(linewidth=0.5)\n",
    "\n",
    "        with xr.set_options(keep_attrs=True):            \n",
    "            var.mean('member_id').plot(color='k', linewidth=1)\n",
    "        plt.title(v)\n",
    "    plt.suptitle(plot_region, fontsize=16, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute temporal means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: this could be parameterized in the notebook\n",
    "with xr.set_options(keep_attrs=True):   \n",
    "    epoch_list = [\n",
    "        ('1920-1950', ds_djf.sel(time=slice('1920', '1950')).mean('time')),\n",
    "        ('2070-2100', ds_djf.sel(time=slice('2070', '2100')).mean('time')),\n",
    "    ]\n",
    "     \n",
    "epoch = xr.DataArray(\n",
    "    [t[0] for t in epoch_list],\n",
    "    dims=('epoch'),\n",
    "    name='epoch',\n",
    ")    \n",
    "\n",
    "ds_djf_epoch = xr.concat(\n",
    "    [t[1] for t in epoch_list], \n",
    "    dim=epoch,\n",
    ")\n",
    "ds_djf_epoch = ds_djf_epoch.compute()\n",
    "ds_djf_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in ds_djf_epoch.epoch.values:\n",
    "\n",
    "    nvar = len(ds_djf_epoch.data_vars)\n",
    "    ncol = int(np.sqrt(nvar))\n",
    "    nrow = int(nvar/ncol) + min(1, nvar%ncol)\n",
    "\n",
    "    fig, ax = plt.subplots(nrow, ncol, figsize=(4*ncol, 3*nrow),\n",
    "                           constrained_layout=True)\n",
    "\n",
    "    for i, v in enumerate(ds_djf_epoch.data_vars):\n",
    "        plt.axes(ax.ravel()[i])\n",
    "\n",
    "        with xr.set_options(keep_attrs=True):             \n",
    "            var = ds_djf_epoch[v].sel(epoch=epoch).mean('member_id')\n",
    "        if 'length' in var.dims:\n",
    "            var = var.sel(length=40.)            \n",
    "        var.plot()\n",
    "        plt.title(v);\n",
    "    plt.suptitle(epoch, fontsize=16, fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.set_options(keep_attrs=True):  \n",
    "    ds_djf_epoch_diff = ds_djf_epoch.diff('epoch').squeeze('epoch')\n",
    "ds_djf_epoch_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvar = len(ds_djf_epoch_diff.data_vars)\n",
    "ncol = int(np.sqrt(nvar))\n",
    "nrow = int(nvar/ncol) + min(1, nvar%ncol)\n",
    "\n",
    "fig, ax = plt.subplots(nrow, ncol, figsize=(4*ncol, 3*nrow),\n",
    "                       constrained_layout=True)\n",
    "\n",
    "for i, v in enumerate(ds_djf_epoch_diff.data_vars):\n",
    "    plt.axes(ax.ravel()[i])\n",
    "\n",
    "    with xr.set_options(keep_attrs=True):             \n",
    "        var = ds_djf_epoch_diff[v].mean('member_id')\n",
    "    if 'length' in var.dims:\n",
    "        var = var.sel(length=40.)            \n",
    "    var.plot()\n",
    "    plt.title(v)\n",
    "\n",
    "epoch = ds_djf_epoch.epoch.data\n",
    "plt.suptitle(f'({epoch[1]}) - ({epoch[0]})', fontsize=16, fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dso_map = {\n",
    "    'data/cesm-le-fields-djf-regional-timeseries.zarr': ds_djf_regional, \n",
    "    'data/cesm-le-fields-djf-epoch-mean.zarr': ds_djf_epoch,  \n",
    "}\n",
    "for file_out, dso in dso_map.items():\n",
    "    util.write_ds_out(dso, file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-funnel]",
   "language": "python",
   "name": "conda-env-miniconda3-funnel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
